{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UpdatedTextClassification",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PESALARAVIKUMAR/Helper2/blob/master/UpdatedTextClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVQSKW4r6vdz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk, string, re, os, time, pandas as pd, numpy as np, emoji, io\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "#nltk.download('stopwords')\n",
        "from nltk.tokenize import (sent_tokenize, word_tokenize, RegexpTokenizer)\n",
        "#nltk.download('punkt')\n",
        "from nltk.stem import (PorterStemmer, LancasterStemmer, SnowballStemmer, WordNetLemmatizer)\n",
        "#nltk.download('wordnet')\n",
        "from collections import Counter\n",
        "from bs4 import BeautifulSoup\n",
        "from emoji.unicode_codes import UNICODE_EMOJI\n",
        "from spellchecker import SpellChecker\n",
        "from google.colab import files\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import (CountVectorizer, TfidfVectorizer)\n",
        "from yellowbrick.text import (FreqDistVisualizer, TSNEVisualizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xq8s6O0AP81c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CHAT_WORDS = \"\"\"\n",
        "AFAIK=As Far As I Know\n",
        "AFK=Away From Keyboard\n",
        "ASAP=As Soon As Possible\n",
        "ATK=At The Keyboard\n",
        "ATM=At The Moment\n",
        "A3=Anytime, Anywhere, Anyplace\n",
        "BAK=Back At Keyboard\n",
        "BBL=Be Back Later\n",
        "BBS=Be Back Soon\n",
        "BFN=Bye For Now\n",
        "B4N=Bye For Now\n",
        "BRB=Be Right Back\n",
        "BRT=Be Right There\n",
        "BTW=By The Way\n",
        "B4=Before\n",
        "B4N=Bye For Now\n",
        "CU=See You\n",
        "CUL8R=See You Later\n",
        "CYA=See You\n",
        "FAQ=Frequently Asked Questions\n",
        "FC=Fingers Crossed\n",
        "FWIW=For What It's Worth\n",
        "FYI=For Your Information\n",
        "GAL=Get A Life\n",
        "GG=Good Game\n",
        "GN=Good Night\n",
        "GMTA=Great Minds Think Alike\n",
        "GR8=Great!\n",
        "G9=Genius\n",
        "IC=I See\n",
        "ICQ=I Seek you (also a chat program)\n",
        "ILU=ILU: I Love You\n",
        "IMHO=In My Honest/Humble Opinion\n",
        "IMO=In My Opinion\n",
        "IOW=In Other Words\n",
        "IRL=In Real Life\n",
        "KISS=Keep It Simple, Stupid\n",
        "LDR=Long Distance Relationship\n",
        "LMAO=Laugh My A.. Off\n",
        "LOL=Laughing Out Loud\n",
        "LTNS=Long Time No See\n",
        "L8R=Later\n",
        "MTE=My Thoughts Exactly\n",
        "M8=Mate\n",
        "NRN=No Reply Necessary\n",
        "OIC=Oh I See\n",
        "PITA=Pain In The A..\n",
        "PRT=Party\n",
        "PRW=Parents Are Watching\n",
        "ROFL=Rolling On The Floor Laughing\n",
        "ROFLOL=Rolling On The Floor Laughing Out Loud\n",
        "ROTFLMAO=Rolling On The Floor Laughing My A.. Off\n",
        "SK8=Skate\n",
        "STATS=Your sex and age\n",
        "ASL=Age, Sex, Location\n",
        "THX=Thank You\n",
        "TTFN=Ta-Ta For Now!\n",
        "TTYL=Talk To You Later\n",
        "U=You\n",
        "U2=You Too\n",
        "U4E=Yours For Ever\n",
        "WB=Welcome Back\n",
        "WTF=What The F...\n",
        "WTG=Way To Go!\n",
        "WUF=Where Are You From?\n",
        "W8=Wait...\n",
        "7K=Sick:-D Laugher\n",
        "\"\"\"\n",
        "\n",
        "CONTRACTION_PATTERNS = [(r'aren\\'t', 'are not'), (r'can\\'t', 'cannot'), (r'couldn\\'t', 'could not'),\n",
        "                        (r'could\\'ve', 'could have'), (r'didn\\'t', 'did not'), (r'doesn\\'t', 'does not'),\n",
        "                        (r'don\\'t', 'do not'), (r'e\\'er', 'ever'), (r'hadn\\'t', 'had not'),\n",
        "                        (r'hasn\\'t', 'has not'), (r'haven\\'t', 'have not'), (r'i\\'m', 'i am'),\n",
        "                        (r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'),\n",
        "                        (r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'),\n",
        "                        (r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'),\n",
        "                        (r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'),\n",
        "                        (r'ain\\'t', 'is not') ]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJqUZQCQuVuM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class text_preprocessing():\n",
        "  # __init__(self, data) used to initialize passed data\n",
        "  def __init__(self, data):\n",
        "    self.data = data\n",
        "\n",
        "  # clean_data(self, data) used to call every preprocessing step by passing data\n",
        "  def clean_data(self):\n",
        "    self.data['Review'] = self.data['Review'].astype(str)\n",
        "    # Duplication controll\n",
        "    self.data = self.data.drop_duplicates(keep='first')\n",
        "    # Converting Chat words\n",
        "    self.chat_words_list, self.chat_words_map_dict = self.get_chat_words_list_dict(CHAT_WORDS)\n",
        "    #self.data['Review'] = self.data['Review'].apply(lambda text: self.convert_chat_words(text, self.chat_words_list, self.chat_words_map_dict))\n",
        "    # Check Spellings\n",
        "    # Combined function for (Converting Chat words & Checking Spellings)\n",
        "    self.data['Review'] = self.data['Review'].apply(lambda text: self.correct_spellings(text))\n",
        "    # Lower casing & replace Emails with emailaddress\n",
        "    self.data['Review'] = self.data['Review'].str.lower()\n",
        "    #self.data['Review'] = self.data['Review'].str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$', 'emailaddress')\n",
        "    # Removing URL's\n",
        "    #self.data['Review'] = self.data['Review'].apply(lambda text: self.remove_urls(text))\n",
        "    # Removing HTML tags\n",
        "    # Combined function for (Removing URL's & HTML tags)\n",
        "    self.data['Review'] = self.data['Review'].apply(lambda text: self.remove_html_tags_with_re(text))\n",
        "    #self.data['Review'] = self.data['Review'].apply(lambda text: self.remove_html_tags_with_BeautifulSoup(text))\n",
        "    # Removing Contractions (what's --> what is, won't --> will not)\n",
        "    #self.data['Review'] = self.data['Review'].apply(lambda text: self.remove_contractions(text))\n",
        "    # Removing Punctuations\n",
        "    # Combined function for (Removing Contraction's & Punctuations)\n",
        "    self.data['Review'] = self.data['Review'].apply(lambda text: self.remove_punctuations(text))\n",
        "    # Replacing Negotiations with Antonmys (not able --> unable)\n",
        "    #self.data['Review'] = self.data['Review'].apply(lambda text: self.replace_negations(text))\n",
        "    # Handle Capitalized Words (use when text not converted to LowerCase)\n",
        "    #self.data['Review'] = self.data['Review'].apply(lambda text: self.handle_capitalized_words(text))\n",
        "    # Removing Stopwords\n",
        "    self.data['Review'] = self.data['Review'].apply(lambda text: self.remove_stopwords(text))\n",
        "    # Removing Frequent Words\n",
        "    #self.data['Review'] = self.data['Review'].apply(lambda text: self.remove_freq_words(text))\n",
        "    # Removing Rare Words\n",
        "    #self.data['Review'] = self.data['Review'].apply(lambda text: self.remove_rare_words(text))\n",
        "    # Stemming\n",
        "    #self.data['Review'] = self.data['Review'].apply(lambda text: self.stem_words(text))\n",
        "    # Lemmatization\n",
        "    #self.data['Review'] = self.data['Review'].apply(lambda text: self.lemmatize_words(text))\n",
        "    # Removing Emojis & Emoticons\n",
        "    #self.data['Review'] = self.data['Review'].apply(lambda text: self.remove_emojis_emoticons(text) if(self.check_emojis_emoticons_present(text)) else text)\n",
        "    # Converting Emojis & Emoticons\n",
        "    self.data['Review'] = self.data['Review'].apply(lambda text: self.convert_emoticons(text) if(self.check_emojis_emoticons_present(text)) else text)            \n",
        "    return self.data\n",
        "\n",
        "  # get_chat_words_list_dict(self, CHAT_WORDS) used to get all chat words (declared above) into list & dictionary\n",
        "  def get_chat_words_list_dict(self, CHAT_WORDS):\n",
        "    chat_words_map_dict = {}\n",
        "    chat_words_list = []\n",
        "    for line in CHAT_WORDS.split(\"\\n\"):\n",
        "        if line != \"\":\n",
        "            cw = line.split(\"=\")[0]\n",
        "            cw_expanded = line.split(\"=\")[1]\n",
        "            chat_words_list.append(cw)\n",
        "            chat_words_map_dict[cw] = cw_expanded\n",
        "    chat_words_list = set(chat_words_list)\n",
        "    return chat_words_list,chat_words_map_dict\n",
        "\n",
        "  #convert_chat_words(self, text, chat_words_list, chat_words_map_dict) used to free the passed text from chat words\n",
        "  def convert_chat_words(self, text, chat_words_list, chat_words_map_dict):\n",
        "    new_text = []\n",
        "    for w in text.split():\n",
        "        if w.upper() in chat_words_list:\n",
        "            new_text.append(chat_words_map_dict[w.upper()])\n",
        "        else:\n",
        "            new_text.append(w)\n",
        "    return \" \".join(new_text)\n",
        "\n",
        "  # correct_spellings(self, text) used to check spellings and correcting them\n",
        "  def correct_spellings(self, text):\n",
        "    spell = SpellChecker()\n",
        "    corrected_text = []\n",
        "    text = self.convert_chat_words(text, self.chat_words_list, self.chat_words_map_dict)\n",
        "    misspelled_words = spell.unknown(text.split())\n",
        "    for word in text.split():\n",
        "        if word in misspelled_words:\n",
        "            corrected_text.append(spell.correction(word))\n",
        "        else:\n",
        "            corrected_text.append(word)\n",
        "    return \" \".join(corrected_text)\n",
        "\n",
        "  #remove_urls(self, text) used to remove URL's in the text\n",
        "  def remove_urls(self, text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)\n",
        "\n",
        "  #remove_html_tags_with_re(self, text) used to remove HTML tags using Regex\n",
        "  def remove_html_tags_with_re(self, text):\n",
        "    html_pattern = re.compile('<.*?>')\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', html_pattern.sub(r'', text))\n",
        "\n",
        "  #remove_html_tags_with_BeautifulSoup(self, text) used to remove HTML tags using BeautifulSoup\n",
        "  def remove_html_tags_with_BeautifulSoup(self, text):\n",
        "    return BeautifulSoup(text, \"lxml\").text\n",
        "\n",
        "  #replace_repetition_punctuations(self, text) used to remove multiple punctuations at a time with some name\n",
        "  def replace_repetition_punctuations(self, text):\n",
        "    #Replaces repetitions of exlamation marks\n",
        "    text = re.sub(r\"(\\!)\\1+\", ' multiExclamation ', text)      \n",
        "    #Replaces repetitions of question marks\n",
        "    text = re.sub(r\"(\\?)\\1+\", ' multiQuestion ', text)      \n",
        "    #Replaces repetitions of stop marks\n",
        "    text = re.sub(r\"(\\.)\\1+\", ' multiStop ', text)\n",
        "    return text\n",
        "\n",
        "  #remove_punctuations(self, text) used to remove punctucations\n",
        "  def remove_punctuations(self, text):\n",
        "    PUNCT_TO_REMOVE = string.punctuation\n",
        "    text = self.remove_contractions(text)\n",
        "    return text.translate(str.maketrans('','',PUNCT_TO_REMOVE))\n",
        "\n",
        "  #remove_contractions(self, text) used to remove contraction\n",
        "  def remove_contractions(self, text):\n",
        "    patterns = [(re.compile(regex), repl) for (regex, repl) in CONTRACTION_PATTERNS]\n",
        "    for (pattern, repl) in patterns:\n",
        "        (text, count) = re.subn(pattern, repl, text)\n",
        "    return text\n",
        "\n",
        "  #replace_negations(self, text) used to replace negations, helping in converting to antonyms from negations\n",
        "  def replace_negations(self, text):\n",
        "    i, l = 0, len(text)\n",
        "    words = []\n",
        "    while i < l:\n",
        "        word = text[i]\n",
        "        if word == 'not' and i+1 < l:\n",
        "            ant = self.replace_to_antonym(text[i+1])\n",
        "            if ant:\n",
        "                words.append(ant)\n",
        "                i += 2\n",
        "                continue\n",
        "        words.append(word)\n",
        "        i += 1\n",
        "    return words\n",
        "\n",
        "  #replace_to_antonym(self, word, pos = None) used to replace negations to antonyms\n",
        "  def replace_to_antonym(self, word, pos = None):\n",
        "    antonyms = set()\n",
        "    for syn in wordnet.synsets(word, pos = pos):\n",
        "        for lemma in syn.lemmas():\n",
        "            for antonym in lemma.antonyms():\n",
        "                antonyms.add(antonym.name())\n",
        "    if len(antonyms) == 1:\n",
        "        return antonyms.pop()\n",
        "    else:\n",
        "        return None\n",
        "    \n",
        "  #handle_capitalized_words(self, text) used to recognize the Capital Words like Abbreviations\n",
        "  def handle_capitalized_words(self, text):\n",
        "    words = word_tokenize(text)\n",
        "    final_text = []\n",
        "    for word in words:\n",
        "        if(len(re.findall(\"[A-Z]{3,}\", word))):\n",
        "            word = word.replace('\\\\', '' )\n",
        "            transformed_word = re.sub(\"[A-Z]{3,}\", \"ALL_CAPS_\"+word, word)\n",
        "            final_text.append(transformed_word)\n",
        "        else:\n",
        "            final_text.append(transformed_word)\n",
        "    return \" \".join(final_text)\n",
        "\n",
        "  #remove_stopwords(self, text) used to remove stopwords in specific language\n",
        "  def remove_stopwords(self, text):\n",
        "    STOPWORDS = set(stopwords.words(\"english\"))\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "\n",
        "  #get_most_freq_words(self, data, n_freq_words) used to get most n_freq_words from text vector spaces\n",
        "  def get_most_freq_words(self, data, n_freq_words):\n",
        "    counter = Counter()\n",
        "    for text in data['Review'].values:\n",
        "        for word in text.split():\n",
        "            counter[word] += 1\n",
        "    return set([w for (w,wc) in counter.most_common(n_freq_words)])\n",
        "    \n",
        "  #remove_freq_words(self, text) used to remove most frequent words in text vector spaces\n",
        "  def remove_freq_words(self, text):\n",
        "    n_freq_words = 10\n",
        "    FREQ_WORDS = self.get_most_freq_words(data,n_freq_words)\n",
        "    return \" \" .join([word for word in str(text).split() if word not in FREQ_WORDS])\n",
        "\n",
        "  #get_most_rare_words(self, data, n_rare_words) used to get most n_rare_words from text vector spaces\n",
        "  def get_most_rare_words(self, data, n_rare_words):\n",
        "    counter = Counter()\n",
        "    for text in data['Review'].values:\n",
        "        for word in text.split():\n",
        "            counter[word] += 1\n",
        "    return set([w for (w,wc) in counter.most_common()[:-n_rare_words-1:-1]])\n",
        "    \n",
        "  #remove_rare_words(self, text) used to remove most rare words in text vector spaces\n",
        "  def remove_rare_words(self, text):\n",
        "    n_rare_words = 10\n",
        "    RARE_WORDS = self.get_most_rare_words(data,n_rare_words)\n",
        "    return \" \" .join([word for word in str(text).split() if word not in RARE_WORDS])\n",
        "\n",
        "  #stem_words(self, text) used to stem the words in text to root form\n",
        "  def stem_words(self, text):\n",
        "    stemmer = PorterStemmer()\n",
        "    return \" \".join(stemmer.stem(word) for word in str(text).split())\n",
        "\n",
        "  #lemmatize_words(self, text) used to lemmatize words\n",
        "  def lemmatize_words(self, text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return \" \".join(lemmatizer.lemmatize(word) for word in str(text).split())\n",
        "\n",
        "  #check_emojis_emoticons_present(self, text) used to check if any emoji or emoticons present in text or not\n",
        "  def check_emojis_emoticons_present(self, text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    if(emoji_pattern.search(text)):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "  #if(check_emojis_emoticons_present(\"game is on ðŸ”¥\")):\n",
        "  #  print(convert_emojis(\"game is on ðŸ”¥\"))\n",
        "  #convert_emojis_emoticons(self, text) used to convert the emojis or emoticons into text form\n",
        "  def convert_emojis_emoticons(self, text):\n",
        "    for emot in UNICODE_EMOJI:\n",
        "        text = re.sub(r'('+emot+')', \"_\".join(UNICODE_EMOJI[emot].replace(\",\",\"\").replace(\":\",\"\").split()), text)\n",
        "    return text\n",
        "\n",
        "  #remove_emojis_emoticons(self, text) used to remove all emojis or emoticons in the text\n",
        "  def remove_emojis_emoticons(self, text):\n",
        "    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in UNICODE_EMOJI) + u')')\n",
        "    return emoticon_pattern.sub(r'', text)\n",
        "\n",
        "  #remove_numbers(self, text) used to remove single numbers\n",
        "  def remove_numbers(self, text):\n",
        "    text = ''.join([i for i in text if not i.isdigit()])         \n",
        "    return text\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HwtJ0r-61EW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqVRr0bk2LNY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(io.StringIO(uploaded['spam_ham_data.csv'].decode('cp1252')),)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep5G0KsD3TZv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f207f7fa-fc64-41ad-f383-e55f4a8643ea"
      },
      "source": [
        "savedData = data\n",
        "data.shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11145, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3WoVYLz2M4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocessing = text_preprocessing(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_UhJ-Vz2eAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = preprocessing.clean_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spi3Hpc07LFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data.shape\n",
        "#data.head()\n",
        "data.to_csv('CleanedTextData.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YS52Ov2LENyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class word_vectorization():\n",
        "  def __init__(self, data):\n",
        "    self.data = data\n",
        "\n",
        "  def BOW_vectorization(self):\n",
        "    count_vectorizer = CountVectorizer()\n",
        "    bow_text = count_vectorizer.fit_transform(self.data['Review'])\n",
        "    bow_feature_names = count_vectorizer.get_feature_names()\n",
        "    visualizations().plot_frequency_distribution(bow_text, bow_feature_names)\n",
        "    X = pd.DataFrame(bow_text.toarray(), columns = bow_feature_names)\n",
        "    Y = data['Liked']\n",
        "    visualizations().plot_TSNE_distribution(bow_text, Y)\n",
        "    return X, Y\n",
        "\n",
        "  def TFIDF_vectorization(self):\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features = 5500)\n",
        "    tfidf_text = tfidf_vectorizer.fit_transform(data['Review'])\n",
        "    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
        "    visualizations().plot_frequency_distribution(tfidf_text, tfidf_feature_names)\n",
        "    X = pd.DataFrame(tfidf_text.toarray(), columns = tfidf_feature_names)\n",
        "    Y = data['Liked']\n",
        "    visualizations().plot_TSNE_distribution(tfidf_text, Y)\n",
        "    return X, Y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTV5nj7iLFgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_embeddings = word_vectorization(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOKCLIbfLSvd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, Y = word_embeddings.BOW_vectorization()\n",
        "#X, Y = word_embedding.TFIDF_vectorization()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esJC_PB1JPdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#X.head()\n",
        "#Y.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikjsyGSfL8ez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size = 0.2, random_state = 12)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxUBEj8YL8h9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (accuracy_score, roc_curve, auc, roc_auc_score, \n",
        "                             confusion_matrix, classification_report)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import GridSearchCV\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nXmsxbTMYcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class machine_learning_models():\n",
        "  def __init__(self, Xtrain, Xtest, Ytrain, Ytest):\n",
        "    self.Xtrain = Xtrain\n",
        "    self.Xtest = Xtest\n",
        "    self.Ytrain = Ytrain\n",
        "    self.Ytest = Ytest\n",
        "\n",
        "  def decision_tree_classification(self):\n",
        "    classifier = DecisionTreeClassifier(\n",
        "            criterion='gini', splitter='best',  max_depth=None, random_state=None, min_samples_split=2, min_samples_leaf=1, max_features=None\n",
        "                                              )\n",
        "    start_time = time.time()\n",
        "    classifier.fit(self.Xtrain, self.Ytrain)\n",
        "    predictions = classifier.predict(self.Xtest)\n",
        "    accuracy = accuracy_score(self.Ytest, predictions)\n",
        "    elapsed_time = (time.time() - start_time)\n",
        "    print('Decision Tree Classification :')\n",
        "    print('Accuracy: {0}'.format(accuracy))\n",
        "    print('Elapsed Time: {0}'.format(elapsed_time))\n",
        "    error_metrics().confusion_matrix(self.Ytest, predictions)\n",
        "    error_metrics().classification_report(self.Ytest, predictions)\n",
        "    #visualizations().plot_roc_auc_curve(self.Ytest, predictions)\n",
        "\n",
        "  def random_forest_classification(self):\n",
        "    classifier = RandomForestClassifier(\n",
        "            criterion='gini', n_estimators=100, max_depth=None, random_state=None, min_samples_split=2, min_samples_leaf=1, max_features=None\n",
        "                                                  )\n",
        "    start_time = time.time()\n",
        "    classifier.fit(self.Xtrain, self.Ytrain)\n",
        "    predictions = classifier.predict(self.Xtest)\n",
        "    accuracy = accuracy_score(self.Ytest, predictions)\n",
        "    elapsed_time = (time.time() - start_time)\n",
        "    print('Random Forest Classification :')\n",
        "    print('Accuracy: {0}'.format(accuracy))\n",
        "    print('Elapsed Time: {0}'.format(elapsed_time))\n",
        "    error_metrics().confusion_matrix(self.Ytest, predictions)\n",
        "    error_metrics().classification_report(self.Ytest, predictions)\n",
        "    #visualizations().plot_roc_auc_curve(self.Ytest, predictions)  \n",
        "\n",
        "  def knn_classification(self):\n",
        "    classifier = KNeighborsClassifier(n_neighbors=5, algorithm='auto',  n_jobs=None)\n",
        "    start_time = time.time()\n",
        "    classifier.fit(self.Xtrain, self.Ytrain)\n",
        "    predictions = classifier.predict(self.Xtest)\n",
        "    accuracy = accuracy_score(self.Ytest, predictions)\n",
        "    elapsed_time = (time.time() - start_time)\n",
        "    print('KNN Classification :')\n",
        "    print('Accuracy: {0}'.format(accuracy))\n",
        "    print('Elapsed Time: {0}'.format(elapsed_time))\n",
        "    error_metrics().confusion_matrix(self.Ytest, predictions)\n",
        "    error_metrics().classification_report(self.Ytest, predictions)\n",
        "    #visualizations().plot_roc_auc_curve(self.Ytest, predictions)  \n",
        "\n",
        "  def multinomialNB_classification(self):\n",
        "    classifier = MultinomialNB(alpha=1.0, fit_prior=False)\n",
        "    start_time = time.time()\n",
        "    classifier.fit(self.Xtrain, self.Ytrain)\n",
        "    predictions = classifier.predict(self.Xtest)\n",
        "    accuracy = accuracy_score(self.Ytest, predictions)\n",
        "    elapsed_time = (time.time() - start_time)\n",
        "    print('MultinomialNB Classification :')\n",
        "    print('Accuracy: {0}'.format(accuracy))\n",
        "    print('Elapsed Time: {0}'.format(elapsed_time))\n",
        "    error_metrics().confusion_matrix(self.Ytest, predictions)\n",
        "    error_metrics().classification_report(self.Ytest, predictions)\n",
        "    #visualizations().plot_roc_auc_curve(self.Ytest, predictions)\n",
        "\n",
        "  def svm_classification(self):\n",
        "    classifier = SVC(kernel='linear', random_state=None, C=1, gamma=1.0)\n",
        "    start_time = time.time()\n",
        "    classifier.fit(self.Xtrain, self.Ytrain)\n",
        "    predictions = classifier.predict(self.Xtest)\n",
        "    accuracy = accuracy_score(self.Ytest, predictions)\n",
        "    elapsed_time = (time.time() - start_time)\n",
        "    print('SVM Classification :')\n",
        "    print('Accuracy: {0}'.format(accuracy))\n",
        "    print('Elapsed Time: {0}'.format(elapsed_time))\n",
        "    error_metrics().confusion_matrix(self.Ytest, predictions)\n",
        "    error_metrics().classification_report(self.Ytest, predictions)\n",
        "    #visualizations().plot_roc_auc_curve(self.Ytest, predictions)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOUlPHy5MYec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ml_models = machine_learning_models(Xtrain, Xtest, Ytrain, Ytest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVvWNze8MYiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ml_models.decision_tree_classification()\n",
        "#ml_models.random_forest_classification()\n",
        "#ml_models.knn_classification()\n",
        "#ml_models.multinomialNB_classification()\n",
        "#ml_models.svm_classification()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds6tYA9xXDPs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class visualizations():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def plot_roc_auc_curve(self, Ytest, Ypred):\n",
        "    fpr, tpr, thresholds = roc_curve(Ytest, Ypred, pos_label = 'T')\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    auc_score = roc_auc_score(Ytest, Ypred)\n",
        "    print('ROC score : {0}'.format(roc_auc))\n",
        "    print('AUC score : {0}'.format(auc_score))\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange')\n",
        "    plt.plot([0,1], [0,1], color='navy', linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.0])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.show()  \n",
        "\n",
        "  # Frequency distribution plots top 50 frequent words\n",
        "  def plot_frequency_distribution(self, text, feature_names):\n",
        "    visualizer = FreqDistVisualizer(features=feature_names)\n",
        "    visualizer.fit(text)\n",
        "    visualizer.poof()\n",
        "  #  t-Distributed Stochastic Neighbor Embedding (t-SNE) is a dimensionality reduction algorithm for visualizing high-dimensional datasets.\n",
        "  def plot_TSNE_distribution(self, text, labels):\n",
        "    tsne = TSNEVisualizer()\n",
        "    tsne.fit_transform(text, labels)\n",
        "    tsne.poof()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyeefSpGMYaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class error_metrics():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def confusion_matrix(self, Ytest, Ypred):\n",
        "    results = confusion_matrix(Ytest, Ypred)\n",
        "    print('Confusion Matrix: {0}'.format(results))\n",
        "    sns.heatmap(results, annot=True, cbar=False)\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "\n",
        "\n",
        "  def classification_report(self, Ytest, Ypred):\n",
        "    report = classification_report(Ytest, Ypred)\n",
        "    print('Classification report: {0}'.format(report))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KksLecPNY4mR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sTWcPdgjoH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}