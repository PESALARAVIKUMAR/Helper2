{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UpdatedTextClassification",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PESALARAVIKUMAR/Helper2/blob/master/UpdatedTextClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVQSKW4r6vdz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "a978f39a-a0bc-4727-a6e5-65da53a8c925"
      },
      "source": [
        "import nltk, string, re, os, time, pandas as pd, numpy as np, emoji, io\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import (sent_tokenize, word_tokenize, RegexpTokenizer)\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import (PorterStemmer, LancasterStemmer, SnowballStemmer, WordNetLemmatizer)\n",
        "nltk.download('wordnet')\n",
        "from collections import Counter\n",
        "from bs4 import BeautifulSoup\n",
        "from emoji.unicode_codes import UNICODE_EMOJI\n",
        "from spellchecker import SpellChecker\n",
        "from google.colab import files"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xq8s6O0AP81c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CHAT_WORDS = \"\"\"\n",
        "AFAIK=As Far As I Know\n",
        "AFK=Away From Keyboard\n",
        "ASAP=As Soon As Possible\n",
        "ATK=At The Keyboard\n",
        "ATM=At The Moment\n",
        "A3=Anytime, Anywhere, Anyplace\n",
        "BAK=Back At Keyboard\n",
        "BBL=Be Back Later\n",
        "BBS=Be Back Soon\n",
        "BFN=Bye For Now\n",
        "B4N=Bye For Now\n",
        "BRB=Be Right Back\n",
        "BRT=Be Right There\n",
        "BTW=By The Way\n",
        "B4=Before\n",
        "B4N=Bye For Now\n",
        "CU=See You\n",
        "CUL8R=See You Later\n",
        "CYA=See You\n",
        "FAQ=Frequently Asked Questions\n",
        "FC=Fingers Crossed\n",
        "FWIW=For What It's Worth\n",
        "FYI=For Your Information\n",
        "GAL=Get A Life\n",
        "GG=Good Game\n",
        "GN=Good Night\n",
        "GMTA=Great Minds Think Alike\n",
        "GR8=Great!\n",
        "G9=Genius\n",
        "IC=I See\n",
        "ICQ=I Seek you (also a chat program)\n",
        "ILU=ILU: I Love You\n",
        "IMHO=In My Honest/Humble Opinion\n",
        "IMO=In My Opinion\n",
        "IOW=In Other Words\n",
        "IRL=In Real Life\n",
        "KISS=Keep It Simple, Stupid\n",
        "LDR=Long Distance Relationship\n",
        "LMAO=Laugh My A.. Off\n",
        "LOL=Laughing Out Loud\n",
        "LTNS=Long Time No See\n",
        "L8R=Later\n",
        "MTE=My Thoughts Exactly\n",
        "M8=Mate\n",
        "NRN=No Reply Necessary\n",
        "OIC=Oh I See\n",
        "PITA=Pain In The A..\n",
        "PRT=Party\n",
        "PRW=Parents Are Watching\n",
        "ROFL=Rolling On The Floor Laughing\n",
        "ROFLOL=Rolling On The Floor Laughing Out Loud\n",
        "ROTFLMAO=Rolling On The Floor Laughing My A.. Off\n",
        "SK8=Skate\n",
        "STATS=Your sex and age\n",
        "ASL=Age, Sex, Location\n",
        "THX=Thank You\n",
        "TTFN=Ta-Ta For Now!\n",
        "TTYL=Talk To You Later\n",
        "U=You\n",
        "U2=You Too\n",
        "U4E=Yours For Ever\n",
        "WB=Welcome Back\n",
        "WTF=What The F...\n",
        "WTG=Way To Go!\n",
        "WUF=Where Are You From?\n",
        "W8=Wait...\n",
        "7K=Sick:-D Laugher\n",
        "\"\"\"\n",
        "\n",
        "CONTRACTION_PATTERNS = [(r'aren\\'t', 'are not'), (r'can\\'t', 'cannot'), (r'couldn\\'t', 'could not'),\n",
        "                        (r'could\\'ve', 'could have'), (r'didn\\'t', 'did not'), (r'doesn\\'t', 'does not'),\n",
        "                        (r'don\\'t', 'do not'), (r'e\\'er', 'ever'), (r'hadn\\'t', 'had not'),\n",
        "                        (r'hasn\\'t', 'has not'), (r'haven\\'t', 'have not'), (r'i\\'m', 'i am'),\n",
        "                        (r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'),\n",
        "                        (r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'),\n",
        "                        (r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'),\n",
        "                        (r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'),\n",
        "                        (r'ain\\'t', 'is not') ]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJqUZQCQuVuM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class text_preprocessing():\n",
        "  # __init__(self, data) used to initialize passed data\n",
        "  def __init__(self, data):\n",
        "    self.data = data\n",
        "\n",
        "  # clean_data(self, data) used to call every preprocessing step by passing data\n",
        "  def clean_data(self):\n",
        "    self.data['Review'] = self.data['Review'].astype(str)\n",
        "    # Duplication controll\n",
        "    #self.data = self.data.drop_duplicates(keep='first')\n",
        "    # Converting Chat words\n",
        "    chat_words_list, chat_words_map_dict = self.get_chat_words_list_dict(CHAT_WORDS)\n",
        "    self.data['Review'] = self.data['Review'].apply(lambda text: self.convert_chat_words(text, chat_words_list, chat_words_map_dict))\n",
        "    # Check Spellings\n",
        "    #self.data['Review'] = self.data['Review'].apply(lambda text: self.correct_spellings(text))\n",
        "    # Lower casing & replace Emails with emailaddress\n",
        "    self.data['Review'] = self.data['Review'].str.lower()\n",
        "    #self.data['Review'] = self.data['Review'].str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$', 'emailaddress')\n",
        "    # Removing URL's\n",
        "    self.data['Review'] = self.data['Review'].apply(lambda text: self.remove_urls(text))\n",
        "    # Removing HTML tags\n",
        "    self.data['Review'] = self.data['Review'].apply(lambda text: self.remove_html_tags_with_re(text))\n",
        "    #self.data['Review'] = self.data['Review'].apply(lambda text: self.remove_html_tags_with_BeautifulSoup(text))\n",
        "    # Removing Punctuations\n",
        "    self.data['Review'] = self.data['Review'].apply(lambda text: self.remove_punctuations(text))\n",
        "    # Removing Contractions (what's --> what is, won't --> will not)\n",
        "    self.data['Review'] = self.data['Review'].apply(lambda text: self.remove_contractions(text))\n",
        "    # Replacing Negotiations with Antonmys (not able --> unable)\n",
        "    #self.data['Review'] = self.data['Review'].apply(lambda text: self.replace_negations(text))\n",
        "    # Handle Capitalized Words (use when text not converted to LowerCase)\n",
        "    #self.data['Review'] = self.data['Review'].apply(lambda text: self.handle_capitalized_words(text))\n",
        "    # Removing Stopwords\n",
        "    self.data['Review'] = self.data['Review'].apply(lambda text: self.remove_stopwords(text))\n",
        "    # Removing Frequent Words\n",
        "    #self.data['Review'] = self.data['Review'].apply(lambda text: self.remove_freq_words(text))\n",
        "    # Removing Rare Words\n",
        "    #self.data['Review'] = self.data['Review'].apply(lambda text: self.remove_rare_words(text))\n",
        "    # Stemming\n",
        "    #self.data['Review'] = self.data['Review'].apply(lambda text: self.stem_words(text))\n",
        "    # Lemmatization\n",
        "    #self.data['Review'] = self.data['Review'].apply(lambda text: self.lemmatize_words(text))\n",
        "    # Removing Emojis & Emoticons\n",
        "    #self.data['Review'] = self.data['Review'].apply(lambda text: self.remove_emojis_emoticons(text) if(self.check_emojis_emoticons_present(text)) else text)\n",
        "    # Converting Emojis & Emoticons\n",
        "    #self.data['Review'] = self.data['Review'].apply(lambda text: self.convert_emoticons(text) if(self.check_emojis_emoticons_present(text)) else text)            \n",
        "    return self.data\n",
        "\n",
        "  # get_chat_words_list_dict(self, CHAT_WORDS) used to get all chat words (declared above) into list & dictionary\n",
        "  def get_chat_words_list_dict(self, CHAT_WORDS):\n",
        "    chat_words_map_dict = {}\n",
        "    chat_words_list = []\n",
        "    for line in CHAT_WORDS.split(\"\\n\"):\n",
        "        if line != \"\":\n",
        "            cw = line.split(\"=\")[0]\n",
        "            cw_expanded = line.split(\"=\")[1]\n",
        "            chat_words_list.append(cw)\n",
        "            chat_words_map_dict[cw] = cw_expanded\n",
        "    chat_words_list = set(chat_words_list)\n",
        "    return chat_words_list,chat_words_map_dict\n",
        "\n",
        "  #convert_chat_words(self, text, chat_words_list, chat_words_map_dict) used to free the passed text from chat words\n",
        "  def convert_chat_words(self, text, chat_words_list, chat_words_map_dict):\n",
        "    new_text = []\n",
        "    for w in text.split():\n",
        "        if w.upper() in chat_words_list:\n",
        "            new_text.append(chat_words_map_dict[w.upper()])\n",
        "        else:\n",
        "            new_text.append(w)\n",
        "    return \" \".join(new_text)\n",
        "\n",
        "  # correct_spellings(self, text) used to check spellings and correcting them\n",
        "  def correct_spellings(self, text):\n",
        "    spell = SpellChecker()\n",
        "    corrected_text = []\n",
        "    misspelled_words = spell.unknown(text.split())\n",
        "    for word in text.split():\n",
        "        if word in misspelled_words:\n",
        "            corrected_text.append(spell.correction(word))\n",
        "        else:\n",
        "            corrected_text.append(word)\n",
        "    return \" \".join(corrected_text)\n",
        "\n",
        "  #remove_urls(self, text) used to remove URL's in the text\n",
        "  def remove_urls(self, text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)\n",
        "\n",
        "  #remove_html_tags_with_re(self, text) used to remove HTML tags using Regex\n",
        "  def remove_html_tags_with_re(self, text):\n",
        "    html_pattern = re.compile('<.*?>')\n",
        "    return html_pattern.sub(r'', text)\n",
        "\n",
        "  #remove_html_tags_with_BeautifulSoup(self, text) used to remove HTML tags using BeautifulSoup\n",
        "  def remove_html_tags_with_BeautifulSoup(self, text):\n",
        "    return BeautifulSoup(text, \"lxml\").text\n",
        "\n",
        "  #remove_punctuations(self, text) used to remove punctucations\n",
        "  def remove_punctuations(self, text):\n",
        "    PUNCT_TO_REMOVE = string.punctuation\n",
        "    return text.translate(str.maketrans('','',PUNCT_TO_REMOVE))\n",
        "\n",
        "  #remove_contractions(self, text) used to remove contraction\n",
        "  def remove_contractions(self, text):\n",
        "    patterns = [(re.compile(regex), repl) for (regex, repl) in CONTRACTION_PATTERNS]\n",
        "    for (pattern, repl) in patterns:\n",
        "        (text, count) = re.subn(pattern, repl, text)\n",
        "    return text\n",
        "\n",
        "  #replace_negations(self, text) used to replace negations, helping in converting to antonyms from negations\n",
        "  def replace_negations(self, text):\n",
        "    i, l = 0, len(text)\n",
        "    words = []\n",
        "    while i < l:\n",
        "        word = text[i]\n",
        "        if word == 'not' and i+1 < l:\n",
        "            ant = self.replace_to_antonym(text[i+1])\n",
        "            if ant:\n",
        "                words.append(ant)\n",
        "                i += 2\n",
        "                continue\n",
        "        words.append(word)\n",
        "        i += 1\n",
        "    return words\n",
        "\n",
        "  #replace_to_antonym(self, word, pos = None) used to replace negations to antonyms\n",
        "  def replace_to_antonym(self, word, pos = None):\n",
        "    antonyms = set()\n",
        "    for syn in wordnet.synsets(word, pos = pos):\n",
        "        for lemma in syn.lemmas():\n",
        "            for antonym in lemma.antonyms():\n",
        "                antonyms.add(antonym.name())\n",
        "    if len(antonyms) == 1:\n",
        "        return antonyms.pop()\n",
        "    else:\n",
        "        return None\n",
        "    \n",
        "  #handle_capitalized_words(self, text) used to recognize the Capital Words like Abbreviations\n",
        "  def handle_capitalized_words(self, text):\n",
        "    words = word_tokenize(text)\n",
        "    final_text = []\n",
        "    for word in words:\n",
        "        if(len(re.findall(\"[A-Z]{3,}\", word))):\n",
        "            word = word.replace('\\\\', '' )\n",
        "            transformed_word = re.sub(\"[A-Z]{3,}\", \"ALL_CAPS_\"+word, word)\n",
        "            final_text.append(transformed_word)\n",
        "        else:\n",
        "            final_text.append(transformed_word)\n",
        "    return \" \".join(final_text)\n",
        "\n",
        "  #remove_stopwords(self, text) used to remove stopwords in specific language\n",
        "  def remove_stopwords(self, text):\n",
        "    STOPWORDS = set(stopwords.words(\"english\"))\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "\n",
        "  #get_most_freq_words(self, data, n_freq_words) used to get most n_freq_words from text vector spaces\n",
        "  def get_most_freq_words(self, data, n_freq_words):\n",
        "    counter = Counter()\n",
        "    for text in data['Review'].values:\n",
        "        for word in text.split():\n",
        "            counter[word] += 1\n",
        "    return set([w for (w,wc) in counter.most_common(n_freq_words)])\n",
        "    \n",
        "  #remove_freq_words(self, text) used to remove most frequent words in text vector spaces\n",
        "  def remove_freq_words(self, text):\n",
        "    n_freq_words = 10\n",
        "    FREQ_WORDS = self.get_most_freq_words(data,n_freq_words)\n",
        "    return \" \" .join([word for word in str(text).split() if word not in FREQ_WORDS])\n",
        "\n",
        "  #get_most_rare_words(self, data, n_rare_words) used to get most n_rare_words from text vector spaces\n",
        "  def get_most_rare_words(self, data, n_rare_words):\n",
        "    counter = Counter()\n",
        "    for text in data['Review'].values:\n",
        "        for word in text.split():\n",
        "            counter[word] += 1\n",
        "    return set([w for (w,wc) in counter.most_common()[:-n_rare_words-1:-1]])\n",
        "    \n",
        "  #remove_rare_words(self, text) used to remove most rare words in text vector spaces\n",
        "  def remove_rare_words(self, text):\n",
        "    n_rare_words = 10\n",
        "    RARE_WORDS = self.get_most_rare_words(data,n_rare_words)\n",
        "    return \" \" .join([word for word in str(text).split() if word not in RARE_WORDS])\n",
        "\n",
        "  #stem_words(self, text) used to stem the words in text to root form\n",
        "  def stem_words(self, text):\n",
        "    stemmer = PorterStemmer()\n",
        "    return \" \".join(stemmer.stem(word) for word in str(text).split())\n",
        "\n",
        "  #lemmatize_words(self, text) used to lemmatize words\n",
        "  def lemmatize_words(self, text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return \" \".join(lemmatizer.lemmatize(word) for word in str(text).split())\n",
        "\n",
        "  #check_emojis_emoticons_present(self, text) used to check if any emoji or emoticons present in text or not\n",
        "  def check_emojis_emoticons_present(self, text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    if(emoji_pattern.search(text)):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "  #if(check_emojis_emoticons_present(\"game is on ðŸ”¥\")):\n",
        "  #  print(convert_emojis(\"game is on ðŸ”¥\"))\n",
        "  #convert_emojis_emoticons(self, text) used to convert the emojis or emoticons into text form\n",
        "  def convert_emojis_emoticons(self, text):\n",
        "    for emot in UNICODE_EMOJI:\n",
        "        text = re.sub(r'('+emot+')', \"_\".join(UNICODE_EMOJI[emot].replace(\",\",\"\").replace(\":\",\"\").split()), text)\n",
        "    return text\n",
        "\n",
        "  #remove_emojis_emoticons(self, text) used to remove all emojis or emoticons in the text\n",
        "  def remove_emojis_emoticons(self, text):\n",
        "    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in UNICODE_EMOJI) + u')')\n",
        "    return emoticon_pattern.sub(r'', text)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HwtJ0r-61EW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqVRr0bk2LNY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(io.StringIO(uploaded['spam_ham_data.csv'].decode('cp1252')),)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep5G0KsD3TZv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b382e9b9-8a14-4297-bb0a-97ea4d193169"
      },
      "source": [
        "savedData = data\n",
        "data.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11145, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3WoVYLz2M4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocessing = text_preprocessing(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_UhJ-Vz2eAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = preprocessing.clean_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spi3Hpc07LFb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7426f702-bc72-4b37-8951-feb2857ef912"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11145, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    }
  ]
}